{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a329471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\PROJEKT ZESPO≈ÅOWY\\venv\\Lib\\site-packages\\pretty_midi\\instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train import train\n",
    "from lofi_model import LofiModel\n",
    "from dataset import MidiDataset\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "from config import *\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dcb974",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebbf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up datasets and dataloaders...\n",
      "Finished setting up datasets and dataloaders.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ziemmi13/lofi-vae-generator/c1d8dec62a0547d8ba8dc165fd812a4f\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "The datset has 963 batches\n",
      "Epoch [1/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 152532.6562\n",
      "\tCurrent training Reconstruction Loss: 152527.0781\n",
      "\tCurrent training KL Loss: 5.5774\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 61430.8252\n",
      "\tCurrent training Reconstruction Loss: 61427.1075\n",
      "\tCurrent training KL Loss: 3.7177\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 43867.1617\n",
      "\tCurrent training Reconstruction Loss: 43865.0763\n",
      "\tCurrent training KL Loss: 2.0856\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 33158.0681\n",
      "\tCurrent training Reconstruction Loss: 33156.6384\n",
      "\tCurrent training KL Loss: 1.4297\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 26972.2283\n",
      "\tCurrent training Reconstruction Loss: 26971.1334\n",
      "\tCurrent training KL Loss: 1.0950\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 22948.2113\n",
      "\tCurrent training Reconstruction Loss: 22947.3246\n",
      "\tCurrent training KL Loss: 0.8868\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 20165.1235\n",
      "\tCurrent training Reconstruction Loss: 20164.3803\n",
      "\tCurrent training KL Loss: 0.7433\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 18094.9904\n",
      "\tCurrent training Reconstruction Loss: 18094.3514\n",
      "\tCurrent training KL Loss: 0.6390\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 16505.3178\n",
      "\tCurrent training Reconstruction Loss: 16504.7569\n",
      "\tCurrent training KL Loss: 0.5610\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 15265.0026\n",
      "\tCurrent training Reconstruction Loss: 15264.5024\n",
      "\tCurrent training KL Loss: 0.5003\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 2592.7079\n",
      "Validation KL Loss: 0.0100\n",
      "Validation Loss: 2592.7179\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [2/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 5256.4326\n",
      "\tCurrent training Reconstruction Loss: 5256.4229\n",
      "\tCurrent training KL Loss: 0.0098\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 4944.7950\n",
      "\tCurrent training Reconstruction Loss: 4944.7877\n",
      "\tCurrent training KL Loss: 0.0073\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 5065.5765\n",
      "\tCurrent training Reconstruction Loss: 5065.5690\n",
      "\tCurrent training KL Loss: 0.0074\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 4962.3297\n",
      "\tCurrent training Reconstruction Loss: 4962.3225\n",
      "\tCurrent training KL Loss: 0.0073\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 4936.5490\n",
      "\tCurrent training Reconstruction Loss: 4936.5410\n",
      "\tCurrent training KL Loss: 0.0080\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 4911.5983\n",
      "\tCurrent training Reconstruction Loss: 4911.5821\n",
      "\tCurrent training KL Loss: 0.0162\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 4855.1032\n",
      "\tCurrent training Reconstruction Loss: 4855.0884\n",
      "\tCurrent training KL Loss: 0.0148\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 4790.8406\n",
      "\tCurrent training Reconstruction Loss: 4790.8271\n",
      "\tCurrent training KL Loss: 0.0135\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 4749.5610\n",
      "\tCurrent training Reconstruction Loss: 4749.5485\n",
      "\tCurrent training KL Loss: 0.0124\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 4713.9675\n",
      "\tCurrent training Reconstruction Loss: 4713.9561\n",
      "\tCurrent training KL Loss: 0.0114\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 2208.8388\n",
      "Validation KL Loss: 0.0012\n",
      "Validation Loss: 2208.8400\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [3/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 3761.1919\n",
      "\tCurrent training Reconstruction Loss: 3761.1909\n",
      "\tCurrent training KL Loss: 0.0010\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 4399.0710\n",
      "\tCurrent training Reconstruction Loss: 4397.6392\n",
      "\tCurrent training KL Loss: 1.4318\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 4320.2359\n",
      "\tCurrent training Reconstruction Loss: 4316.3422\n",
      "\tCurrent training KL Loss: 3.8937\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 4247.8327\n",
      "\tCurrent training Reconstruction Loss: 4244.8184\n",
      "\tCurrent training KL Loss: 3.0143\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 4267.5202\n",
      "\tCurrent training Reconstruction Loss: 4265.2235\n",
      "\tCurrent training KL Loss: 2.2967\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 4177.7309\n",
      "\tCurrent training Reconstruction Loss: 4175.8742\n",
      "\tCurrent training KL Loss: 1.8567\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 4101.2611\n",
      "\tCurrent training Reconstruction Loss: 4099.7046\n",
      "\tCurrent training KL Loss: 1.5564\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 4021.7084\n",
      "\tCurrent training Reconstruction Loss: 4020.3690\n",
      "\tCurrent training KL Loss: 1.3394\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 3994.6052\n",
      "\tCurrent training Reconstruction Loss: 3993.4291\n",
      "\tCurrent training KL Loss: 1.1761\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 3947.8024\n",
      "\tCurrent training Reconstruction Loss: 3946.7543\n",
      "\tCurrent training KL Loss: 1.0481\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 1934.2230\n",
      "Validation KL Loss: 0.0146\n",
      "Validation Loss: 1934.2375\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [4/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 2913.1438\n",
      "\tCurrent training Reconstruction Loss: 2913.1267\n",
      "\tCurrent training KL Loss: 0.0170\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 3264.5955\n",
      "\tCurrent training Reconstruction Loss: 3264.5803\n",
      "\tCurrent training KL Loss: 0.0153\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 3156.0886\n",
      "\tCurrent training Reconstruction Loss: 3156.0746\n",
      "\tCurrent training KL Loss: 0.0140\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 3114.9320\n",
      "\tCurrent training Reconstruction Loss: 3114.9191\n",
      "\tCurrent training KL Loss: 0.0129\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 3063.8487\n",
      "\tCurrent training Reconstruction Loss: 3063.8367\n",
      "\tCurrent training KL Loss: 0.0121\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 3017.6292\n",
      "\tCurrent training Reconstruction Loss: 3017.6179\n",
      "\tCurrent training KL Loss: 0.0113\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 2967.1801\n",
      "\tCurrent training Reconstruction Loss: 2967.1695\n",
      "\tCurrent training KL Loss: 0.0106\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 3430.7182\n",
      "\tCurrent training Reconstruction Loss: 3430.7082\n",
      "\tCurrent training KL Loss: 0.0100\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 3313.5487\n",
      "\tCurrent training Reconstruction Loss: 3313.5393\n",
      "\tCurrent training KL Loss: 0.0094\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 3234.2920\n",
      "\tCurrent training Reconstruction Loss: 3234.2831\n",
      "\tCurrent training KL Loss: 0.0090\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 1594.1762\n",
      "Validation KL Loss: 0.0033\n",
      "Validation Loss: 1594.1795\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [5/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 2249.7258\n",
      "\tCurrent training Reconstruction Loss: 2249.7217\n",
      "\tCurrent training KL Loss: 0.0042\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 2248.0805\n",
      "\tCurrent training Reconstruction Loss: 2248.0760\n",
      "\tCurrent training KL Loss: 0.0044\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 2329.3121\n",
      "\tCurrent training Reconstruction Loss: 2329.3079\n",
      "\tCurrent training KL Loss: 0.0042\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 2250.7284\n",
      "\tCurrent training Reconstruction Loss: 2250.7243\n",
      "\tCurrent training KL Loss: 0.0040\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 2302.2809\n",
      "\tCurrent training Reconstruction Loss: 2302.2770\n",
      "\tCurrent training KL Loss: 0.0039\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 2297.2842\n",
      "\tCurrent training Reconstruction Loss: 2297.2805\n",
      "\tCurrent training KL Loss: 0.0037\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 2254.0112\n",
      "\tCurrent training Reconstruction Loss: 2254.0077\n",
      "\tCurrent training KL Loss: 0.0036\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 2234.4710\n",
      "\tCurrent training Reconstruction Loss: 2234.4676\n",
      "\tCurrent training KL Loss: 0.0034\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 2208.2321\n",
      "\tCurrent training Reconstruction Loss: 2208.2288\n",
      "\tCurrent training KL Loss: 0.0033\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 2759.8871\n",
      "\tCurrent training Reconstruction Loss: 2759.8839\n",
      "\tCurrent training KL Loss: 0.0032\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 1425.0469\n",
      "Validation KL Loss: 0.0013\n",
      "Validation Loss: 1425.0482\n",
      "____________________________________________________________ \n",
      "\n",
      "Finished training!\n",
      "Saving the model to path: ./saved_models/multitrack LSTM-VAE (1 min).pth\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "    # Dataset setup\n",
    "    dataset_dir = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\dataset\\transformed_dataset\"\n",
    "\n",
    "    # Model setup\n",
    "    model = LofiModel(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train\n",
    "    train(model, \n",
    "          dataset_dir, \n",
    "          experiment_name=\"First try multitrack, transformed dataset\",\n",
    "          verbose=True, \n",
    "          model_save_path = \"./saved_models/multitrack LSTM-VAE (1 min).pth\")\n",
    "    \n",
    "# 5 epok to 209 minut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c883c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LofiModel(\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(640, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (hidden_to_mu): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (hidden_to_logvar): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent_to_hidden): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (latent_to_cell): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (lstm): LSTM(640, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (fc_out): Linear(in_features=256, out_features=640, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = LofiModel(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"./saved_models/multitrack LSTM-VAE (1 min).pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4aef5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3582080"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
