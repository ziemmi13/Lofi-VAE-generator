{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a329471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\venv\\Lib\\site-packages\\pretty_midi\\instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train import train\n",
    "from lofi_model import LofiModel\n",
    "from dataset import MidiDataset\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "from config import *\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dcb974",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebbf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up datasets and dataloaders...\n",
      "Finished setting up datasets and dataloaders.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ziemmi13/lofi-vae-generator/c1d8dec62a0547d8ba8dc165fd812a4f\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training:\n",
      "The datset has 963 batches\n",
      "Epoch [1/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 152532.6562\n",
      "\tCurrent training Reconstruction Loss: 152527.0781\n",
      "\tCurrent training KL Loss: 5.5774\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 61430.8252\n",
      "\tCurrent training Reconstruction Loss: 61427.1075\n",
      "\tCurrent training KL Loss: 3.7177\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 43867.1617\n",
      "\tCurrent training Reconstruction Loss: 43865.0763\n",
      "\tCurrent training KL Loss: 2.0856\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 33158.0681\n",
      "\tCurrent training Reconstruction Loss: 33156.6384\n",
      "\tCurrent training KL Loss: 1.4297\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 26972.2283\n",
      "\tCurrent training Reconstruction Loss: 26971.1334\n",
      "\tCurrent training KL Loss: 1.0950\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 22948.2113\n",
      "\tCurrent training Reconstruction Loss: 22947.3246\n",
      "\tCurrent training KL Loss: 0.8868\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 20165.1235\n",
      "\tCurrent training Reconstruction Loss: 20164.3803\n",
      "\tCurrent training KL Loss: 0.7433\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 18094.9904\n",
      "\tCurrent training Reconstruction Loss: 18094.3514\n",
      "\tCurrent training KL Loss: 0.6390\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 16505.3178\n",
      "\tCurrent training Reconstruction Loss: 16504.7569\n",
      "\tCurrent training KL Loss: 0.5610\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 15265.0026\n",
      "\tCurrent training Reconstruction Loss: 15264.5024\n",
      "\tCurrent training KL Loss: 0.5003\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 2592.7079\n",
      "Validation KL Loss: 0.0100\n",
      "Validation Loss: 2592.7179\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [2/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 5256.4326\n",
      "\tCurrent training Reconstruction Loss: 5256.4229\n",
      "\tCurrent training KL Loss: 0.0098\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 4944.7950\n",
      "\tCurrent training Reconstruction Loss: 4944.7877\n",
      "\tCurrent training KL Loss: 0.0073\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 5065.5765\n",
      "\tCurrent training Reconstruction Loss: 5065.5690\n",
      "\tCurrent training KL Loss: 0.0074\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 4962.3297\n",
      "\tCurrent training Reconstruction Loss: 4962.3225\n",
      "\tCurrent training KL Loss: 0.0073\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 4936.5490\n",
      "\tCurrent training Reconstruction Loss: 4936.5410\n",
      "\tCurrent training KL Loss: 0.0080\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 4911.5983\n",
      "\tCurrent training Reconstruction Loss: 4911.5821\n",
      "\tCurrent training KL Loss: 0.0162\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 4855.1032\n",
      "\tCurrent training Reconstruction Loss: 4855.0884\n",
      "\tCurrent training KL Loss: 0.0148\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 4790.8406\n",
      "\tCurrent training Reconstruction Loss: 4790.8271\n",
      "\tCurrent training KL Loss: 0.0135\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 4749.5610\n",
      "\tCurrent training Reconstruction Loss: 4749.5485\n",
      "\tCurrent training KL Loss: 0.0124\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 4713.9675\n",
      "\tCurrent training Reconstruction Loss: 4713.9561\n",
      "\tCurrent training KL Loss: 0.0114\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 2208.8388\n",
      "Validation KL Loss: 0.0012\n",
      "Validation Loss: 2208.8400\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [3/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 3761.1919\n",
      "\tCurrent training Reconstruction Loss: 3761.1909\n",
      "\tCurrent training KL Loss: 0.0010\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 4399.0710\n",
      "\tCurrent training Reconstruction Loss: 4397.6392\n",
      "\tCurrent training KL Loss: 1.4318\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 4320.2359\n",
      "\tCurrent training Reconstruction Loss: 4316.3422\n",
      "\tCurrent training KL Loss: 3.8937\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 4247.8327\n",
      "\tCurrent training Reconstruction Loss: 4244.8184\n",
      "\tCurrent training KL Loss: 3.0143\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 4267.5202\n",
      "\tCurrent training Reconstruction Loss: 4265.2235\n",
      "\tCurrent training KL Loss: 2.2967\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 4177.7309\n",
      "\tCurrent training Reconstruction Loss: 4175.8742\n",
      "\tCurrent training KL Loss: 1.8567\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 4101.2611\n",
      "\tCurrent training Reconstruction Loss: 4099.7046\n",
      "\tCurrent training KL Loss: 1.5564\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 4021.7084\n",
      "\tCurrent training Reconstruction Loss: 4020.3690\n",
      "\tCurrent training KL Loss: 1.3394\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 3994.6052\n",
      "\tCurrent training Reconstruction Loss: 3993.4291\n",
      "\tCurrent training KL Loss: 1.1761\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 3947.8024\n",
      "\tCurrent training Reconstruction Loss: 3946.7543\n",
      "\tCurrent training KL Loss: 1.0481\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 1934.2230\n",
      "Validation KL Loss: 0.0146\n",
      "Validation Loss: 1934.2375\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [4/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 2913.1438\n",
      "\tCurrent training Reconstruction Loss: 2913.1267\n",
      "\tCurrent training KL Loss: 0.0170\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 3264.5955\n",
      "\tCurrent training Reconstruction Loss: 3264.5803\n",
      "\tCurrent training KL Loss: 0.0153\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 3156.0886\n",
      "\tCurrent training Reconstruction Loss: 3156.0746\n",
      "\tCurrent training KL Loss: 0.0140\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 3114.9320\n",
      "\tCurrent training Reconstruction Loss: 3114.9191\n",
      "\tCurrent training KL Loss: 0.0129\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 3063.8487\n",
      "\tCurrent training Reconstruction Loss: 3063.8367\n",
      "\tCurrent training KL Loss: 0.0121\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 3017.6292\n",
      "\tCurrent training Reconstruction Loss: 3017.6179\n",
      "\tCurrent training KL Loss: 0.0113\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 2967.1801\n",
      "\tCurrent training Reconstruction Loss: 2967.1695\n",
      "\tCurrent training KL Loss: 0.0106\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 3430.7182\n",
      "\tCurrent training Reconstruction Loss: 3430.7082\n",
      "\tCurrent training KL Loss: 0.0100\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 3313.5487\n",
      "\tCurrent training Reconstruction Loss: 3313.5393\n",
      "\tCurrent training KL Loss: 0.0094\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 3234.2920\n",
      "\tCurrent training Reconstruction Loss: 3234.2831\n",
      "\tCurrent training KL Loss: 0.0090\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 1594.1762\n",
      "Validation KL Loss: 0.0033\n",
      "Validation Loss: 1594.1795\n",
      "____________________________________________________________ \n",
      "\n",
      "Epoch [5/5]\n",
      "Training:\n",
      "\tBatch index: 1/963\n",
      "\tCurrent training Loss: 2249.7258\n",
      "\tCurrent training Reconstruction Loss: 2249.7217\n",
      "\tCurrent training KL Loss: 0.0042\n",
      "\tBatch index: 101/963\n",
      "\tCurrent training Loss: 2248.0805\n",
      "\tCurrent training Reconstruction Loss: 2248.0760\n",
      "\tCurrent training KL Loss: 0.0044\n",
      "\tBatch index: 201/963\n",
      "\tCurrent training Loss: 2329.3121\n",
      "\tCurrent training Reconstruction Loss: 2329.3079\n",
      "\tCurrent training KL Loss: 0.0042\n",
      "\tBatch index: 301/963\n",
      "\tCurrent training Loss: 2250.7284\n",
      "\tCurrent training Reconstruction Loss: 2250.7243\n",
      "\tCurrent training KL Loss: 0.0040\n",
      "\tBatch index: 401/963\n",
      "\tCurrent training Loss: 2302.2809\n",
      "\tCurrent training Reconstruction Loss: 2302.2770\n",
      "\tCurrent training KL Loss: 0.0039\n",
      "\tBatch index: 501/963\n",
      "\tCurrent training Loss: 2297.2842\n",
      "\tCurrent training Reconstruction Loss: 2297.2805\n",
      "\tCurrent training KL Loss: 0.0037\n",
      "\tBatch index: 601/963\n",
      "\tCurrent training Loss: 2254.0112\n",
      "\tCurrent training Reconstruction Loss: 2254.0077\n",
      "\tCurrent training KL Loss: 0.0036\n",
      "\tBatch index: 701/963\n",
      "\tCurrent training Loss: 2234.4710\n",
      "\tCurrent training Reconstruction Loss: 2234.4676\n",
      "\tCurrent training KL Loss: 0.0034\n",
      "\tBatch index: 801/963\n",
      "\tCurrent training Loss: 2208.2321\n",
      "\tCurrent training Reconstruction Loss: 2208.2288\n",
      "\tCurrent training KL Loss: 0.0033\n",
      "\tBatch index: 901/963\n",
      "\tCurrent training Loss: 2759.8871\n",
      "\tCurrent training Reconstruction Loss: 2759.8839\n",
      "\tCurrent training KL Loss: 0.0032\n",
      "Validating:\n",
      "Validation Reconstruction Loss: 1425.0469\n",
      "Validation KL Loss: 0.0013\n",
      "Validation Loss: 1425.0482\n",
      "____________________________________________________________ \n",
      "\n",
      "Finished training!\n",
      "Saving the model to path: ./saved_models/multitrack LSTM-VAE (1 min).pth\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "    # Dataset setup\n",
    "    dataset_dir = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\dataset\\transformed_dataset\"\n",
    "\n",
    "    # Model setup\n",
    "    model = LofiModel(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train\n",
    "    train(model, \n",
    "          dataset_dir, \n",
    "          experiment_name=\"First try multitrack, transformed dataset\",\n",
    "          verbose=True, \n",
    "          model_save_path = \"./saved_models/multitrack LSTM-VAE (1 min).pth\")\n",
    "    \n",
    "# 5 epok to 209 minut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c883c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LofiModel(\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(640, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (hidden_to_mu): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (hidden_to_logvar): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent_to_hidden): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (latent_to_cell): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (lstm): LSTM(640, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (fc_out): Linear(in_features=256, out_features=640, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = LofiModel(device)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"./saved_models/multitrack LSTM-VAE (1 min).pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cad895",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6fac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\dataset\\transformed_dataset\"\n",
    "dataset = MidiDataset(dataset_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfda65cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed multi-instrument MIDI saved to reconstructions/cc57f2da27012c4ee777fe667db60647.mid-reconstructed.mid\n",
      "Reconstructed multi-instrument MIDI saved to reconstructions/01a400f01c65a687860801daf1320942.mid-reconstructed.mid\n",
      "Reconstructed multi-instrument MIDI saved to reconstructions/43e39bff9abbb242baf5372d4486bee3.mid-reconstructed.mid\n",
      "Reconstructed multi-instrument MIDI saved to reconstructions/928fe4af351f39d51735d7f8652ecb31.mid-reconstructed.mid\n",
      "Reconstructed multi-instrument MIDI saved to reconstructions/428c487a54ea907103f0e80eb3359d89.mid-reconstructed.mid\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    original_sequence_tensor, original_length_val, bpm, filename = dataset[i]\n",
    "    reconstructions = model.reconstruct(original_sequence_tensor, bpm, save_path=f\"reconstructions/{filename}-reconstructed.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ae2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a001b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3a6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c328b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d0e1618",
   "metadata": {},
   "source": [
    "# CHORD EXTRACTOR DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b52459d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Cannot find file in C:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\dataset\\1 2 6 - chord progression.mid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmusic21\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m converter\n\u001b[32m      3\u001b[39m midi_pth = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mHyperbook\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSTUDIA\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSEM III\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mProjekt zespolowy\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1 2 6 - chord progression.mid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m midi_file = \u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmidi_pth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m chords = midi_file.chordify()\n\u001b[32m      8\u001b[39m chord_progression = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\venv\\Lib\\site-packages\\music21\\converter\\__init__.py:1422\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(value, forceSource, number, format, **keywords)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCannot find file in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   1420\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m common.findFormatFile(value) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1421\u001b[39m     \u001b[38;5;66;03m# assume mistyped file path\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCannot find file in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1424\u001b[39m     \u001b[38;5;66;03m# all else, including MidiBytes\u001b[39;00m\n\u001b[32m   1425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parseData(value, number=number, \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m, **keywords)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Cannot find file in C:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\dataset\\1 2 6 - chord progression.mid"
     ]
    }
   ],
   "source": [
    "from music21 import converter\n",
    "\n",
    "midi_pth = r\"C:\\Users\\Hyperbook\\Desktop\\STUDIA\\SEM III\\Projekt zespolowy\\dataset\\1 2 6 - chord progression.mid\"\n",
    "midi_file = converter.parse(midi_pth)\n",
    "\n",
    "chords = midi_file.chordify()\n",
    "\n",
    "chord_progression = []\n",
    "\n",
    "for c in chords.flat.getElementsByClass(\"Chord\"):\n",
    "    if not c.isRest:\n",
    "        root_note = c.root().name\n",
    "        quality = c.quality\n",
    "        full_chord_name = c.pitchedCommonName\n",
    "        offset = c.offset  \n",
    "\n",
    "        chord_progression.append([root_note, quality, full_chord_name, offset])\n",
    "\n",
    "chord_progression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array = np.load(\"../midi/98f90636c139256f4b7dade28ab87088.npz\")\n",
    "print(array)\n",
    "for a in array.keys():\n",
    "    print(a)\n",
    "    print(array[a])\n",
    "    print(20*\"___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_all_tracks_from_npz(npz_path):\n",
    "    \"\"\"\n",
    "    Load all sparse pianoroll tracks from an LPD .npz file into dense arrays.\n",
    "\n",
    "    Returns:\n",
    "        dict[int, np.ndarray]: Mapping from track_id to dense pianoroll (shape: time x pitch)\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    tracks = {}\n",
    "\n",
    "    for key in data.files:\n",
    "        if key.endswith(\"_csc_shape\"):\n",
    "            track_id = int(key.split(\"_\")[1])\n",
    "            shape = data[f\"pianoroll_{track_id}_csc_shape\"]\n",
    "            indptr = data[f\"pianoroll_{track_id}_csc_indptr\"]\n",
    "            indices = data[f\"pianoroll_{track_id}_csc_indices\"]\n",
    "\n",
    "            if f\"pianoroll_{track_id}_csc_data\" in data:\n",
    "                values = data[f\"pianoroll_{track_id}_csc_data\"]\n",
    "            else:\n",
    "                values = np.ones_like(indices, dtype=np.uint8)\n",
    "\n",
    "            sparse = csc_matrix((values, indices, indptr), shape=shape)\n",
    "            dense = sparse.toarray()\n",
    "            tracks[track_id] = dense\n",
    "\n",
    "    return tracks\n",
    "\n",
    "def plot_all_tracks(tracks_dict):\n",
    "    \"\"\"\n",
    "    Plot all tracks in the same figure, stacked vertically.\n",
    "    Each track is shown as a separate pianoroll.\n",
    "    \"\"\"\n",
    "    num_tracks = len(tracks_dict)\n",
    "    fig, axes = plt.subplots(num_tracks, 1, figsize=(12, 2.5 * num_tracks), sharex=True)\n",
    "\n",
    "    if num_tracks == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, (track_id, pianoroll) in enumerate(sorted(tracks_dict.items())):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(pianoroll.T, aspect='auto', origin='lower', cmap='Greys')\n",
    "        ax.set_title(f\"Track {track_id}\")\n",
    "        ax.set_ylabel(\"Pitch\")\n",
    "        ax.set_xlabel(\"Time step\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c672d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = \"../midi/ea04f8ebe8f52d78ab6ce59d9ba04d20.npz\"\n",
    "\n",
    "tracks = load_all_tracks_from_npz(npz_path)\n",
    "plot_all_tracks(tracks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = \"../midi/98f90636c139256f4b7dade28ab87088.npz\"\n",
    "\n",
    "tracks = load_all_tracks_from_npz(npz_path)\n",
    "plot_all_tracks(tracks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
